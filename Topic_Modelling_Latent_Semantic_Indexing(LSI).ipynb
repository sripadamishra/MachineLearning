{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by1No4lFGu8l"
      },
      "outputs": [],
      "source": [
        "#TOPIC Modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
        "!tar -xzf nips12raw_str602.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PV8pH_rG1Fl",
        "outputId": "edbbe022-cf20-41c3-ecd4-b78ba0ed8a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-07 14:53:01--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
            "Resolving cs.nyu.edu (cs.nyu.edu)... 216.165.22.203\n",
            "Connecting to cs.nyu.edu (cs.nyu.edu)|216.165.22.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12851423 (12M) [application/x-gzip]\n",
            "Saving to: ‘nips12raw_str602.tgz’\n",
            "\n",
            "nips12raw_str602.tg 100%[===================>]  12.26M  22.8MB/s    in 0.5s    \n",
            "\n",
            "2023-05-07 14:53:02 (22.8 MB/s) - ‘nips12raw_str602.tgz’ saved [12851423/12851423]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW8gHOOlHBLw",
        "outputId": "312deb50-ab5a-480a-aa92-646399f8d1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = 'nipstxt/'\n",
        "print(os.listdir(DATA_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYPfk175HOsz",
        "outputId": "783dd59f-d89c-4c61-88f4-45fa8aa8badd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RAW_DATA_NOTES', 'MATLAB_NOTES', 'idx', 'nips07', 'nips04', 'nips10', 'nips11', 'orig', 'nips02', 'nips12', 'nips05', 'nips03', 'nips01', 'nips08', 'nips06', 'nips00', 'nips09', 'README_yann']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
        "papers = []\n",
        "for folder in folders:\n",
        "  file_names = os.listdir(DATA_PATH + folder)\n",
        "  for file_name in file_names:\n",
        "    file_name = DATA_PATH + folder + '/' + file_name\n",
        "    with open(file_name,encoding='utf-8',mode='r+',errors='ignore') as file:\n",
        "      text = file.read()\n",
        "      papers.append(text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLt0hmkWHd_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(papers))\n",
        "print(papers[100][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr20IVwAgxMo",
        "outputId": "e9268eec-aab0-4924-eac1-b0657329528e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1740\n",
            "671 \n",
            "PROGR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let us do Basic Text Pre-processing"
      ],
      "metadata": {
        "id": "XC_zQmX3jUeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import nltk\n",
        "import tqdm\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(papers):\n",
        "  normalized_papers = []\n",
        "  for paper in tqdm.tqdm(papers):\n",
        "    paper = paper.lower()\n",
        "    paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "    paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "    paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "    paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "    paper_tokens = list(filter(None,paper_tokens))\n",
        "    if paper_tokens:\n",
        "      normalized_papers.append(paper_tokens)\n",
        "  \n",
        "  return normalized_papers\n",
        "\n",
        "\n",
        "norm_papers = normalize_text(papers)\n",
        "\n",
        "print(norm_papers[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG9Ad0nljYaW",
        "outputId": "65dc579b-d0ae-47e5-9a17-dbdc81145762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1740/1740 [00:35<00:00, 48.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['patyep', 'class', 'degeneracy', 'unrestricted', 'storage', 'density', 'memory', 'christopher', 'scofield', 'douglas', 'reilly', 'charles', 'elbaum', 'leon', 'cooper', 'nestor', 'inc', 'richmond', 'square', 'providence', 'rhode', 'island', 'abstract', 'study', 'distributed', 'memory', 'system', 'ha', 'produced', 'number', 'model', 'work', 'well', 'limited', 'domain', 'however', 'recently', 'application', 'system', 'real', 'world', 'problem', 'ha', 'difficult', 'storage', 'limitation', 'inherent', 'architectural', 'serial', 'simulation', 'computational', 'complexity', 'recent', 'development', 'memory', 'unrestricted', 'storage', 'capacity', 'economical', 'feedforward', 'architecture', 'ha', 'opened', 'way', 'application', 'system', 'complex', 'pattern', 'recognition', 'problem', 'however', 'problem', 'sometimes', 'underspecified', 'feature', 'describe', 'environment', 'thus', 'significant', 'portion', 'pattern', 'environment', 'often', 'non', 'separable', 'review', 'current', 'work', 'high', 'density', 'memory', 'system', 'network', 'implementation', 'discus', 'general', 'learning', 'algorithm', 'high', 'density', 'memory', 'review', 'application', 'separable', 'point', 'set', 'finally', 'introduce', 'extension', 'method', 'learning', 'probability', 'distribution', 'non', 'separable', 'point', 'set', 'introduction', 'information', 'storage', 'distributed', 'content', 'addressable', 'memory', 'ha', 'long', 'topic', 'intense', 'study', 'early', 'research', 'focused', 'development', 'correlation', 'matrix', 'memory', 'worker', 'field', 'found', 'memory', 'sort', 'allowed', 'storage', 'number', 'distinct', 'memory', 'larger', 'number', 'dimension', 'input', 'space', 'storage', 'beyond', 'number', 'caused', 'system', 'give', 'incorrect', 'output', 'memorized', 'input', 'american', 'institute', 'physic', 'recent', 'work', 'distributed', 'memory', 'system', 'ha', 'focused', 'single', 'layer', 'recurrent', 'network', 'hopfield', '6introduced', 'method', 'analysis', 'settling', 'activity', 'recurrent', 'network', 'method', 'defined', 'network', 'dynamical', 'system', 'global', 'function', 'called', 'energy', 'actually', 'liapunov', 'function', 'autonomous', 'system', 'describing', 'state', 'network', 'could', 'defined', 'hopfield', 'showed', 'flow', 'state', 'space', 'always', 'toward', 'fixed', 'point', 'dynamical', 'system', 'matrix', 'recurrent', 'connection', 'satisfies', 'certain', 'condition', 'property', 'hopfield', 'wa', 'able', 'define', 'fixed', 'point', 'site', 'memory', 'network', 'activity', 'like', 'forerunner', 'hopfield', 'network', 'limited', 'storage', 'capacity', 'empirical', 'study', 'system', 'found', 'randomly', 'chosen', 'memory', 'storage', 'capacity', 'wa', 'limited', '15n', 'number', 'memory', 'could', 'accurately', 'recalled', 'dimensionality', 'network', 'ha', 'since', 'improved', 'degradation', 'memory', 'recall', 'increased', 'storage', 'density', 'directly', 'related', 'proliferation', 'state', 'space', 'unwanted', 'local', 'minimum', 'serve', 'basin', 'flow', 'unrestricted', 'storage', 'density', 'memory', 'bachman', 'et', 'al', 'studied', 'another', 'relaxation', 'system', 'similar', 'respect', 'hopfield', 'network', 'however', 'contrast', 'hopfield', 'focused', 'defining', 'dynamical', 'system', 'location', 'minimum', 'explicitly', 'known', 'particular', 'chosen', 'system', 'liapunov', 'function', 'given', 'lz', 'qj', 'xjl', 'total', 'energy', 'network', 'vector', 'describing', 'initial', 'network', 'activity', 'caused', 'test', 'pattern', 'xj', 'site', 'jth', 'memory', 'memory', 'parameter', 'related', 'network', 'size', 'relaxes', 'xj', 'memory', 'according', 'system', 'isomorphic', 'classical', 'electrostatic', 'potential', 'positive', 'unit', 'test', 'charge', 'negative', 'charge', 'qj', 'site', 'xj', 'dimensional', 'input', 'space', 'dimensional', 'coulomb', 'energy', 'function', 'defines', 'exactly', 'basin', 'attraction', 'fixed', 'point', 'located', 'charge', 'site', 'xj', 'shown', 'convergence', 'closest', 'distinct', 'memory', 'guaranteed', 'independent', 'number', 'stored', 'memory', 'proper', 'choice', 'equation', 'show', 'cell', 'receives', 'feedback', 'network', 'form', 'scalar', 'qj', 'xj', 'importantly', 'quantity', 'cell', 'single', 'virtual', 'cell', 'wa', 'computing', 'distance', 'activity', 'space', 'current', 'state', 'stored', 'state', 'result', 'computation', 'broadcast', 'cell', 'network', 'layer', 'feedforward', 'network', 'implementing', 'system', 'ha', 'described', 'elsewhere10', 'connectivity', 'architecture', 'order', 'number', 'stored', 'memory', 'dimensionality', 'layer', 'significant', 'since', 'addition', 'new', 'memory', 'change', 'connectivity', 'addition', 'connection', 'whereas', 'hopfield', 'network', 'addition', 'new', 'memory', 'requires', 'addition', '2n', 'connection', 'equilibrium', 'feedforward', 'network', 'similar', 'property', 'ha', 'investigation', 'time', 'model', 'doe', 'employ', 'relaxation', 'procedure', 'thus', 'wa', 'originally', 'framed', 'language', 'liapunov', 'function', 'however', 'possible', 'define', 'similar', 'system', 'identify', 'location', 'prot6types', 'model', 'location', 'state', 'space', 'potential', 'satisfy', 'following', 'condition', 'ej', 'qj', 'xj', 'xj', 'constant', 'form', 'potential', 'often', 'referred', 'square', 'well', 'potential', 'potential', 'may', 'viewed', 'limit', 'dimensional', 'coulomb', 'potential', 'well', 'replaced', 'square', 'well', 'equation', 'describes', 'energy', 'landscape', 'consists', 'plateau', 'zero', 'potential', 'outside', 'well', 'flat', 'zero', 'slope', 'basin', 'since', 'landscape', 'ha', 'flat', 'region', 'separated', 'discontinuous', 'boundary', 'state', 'network', 'always', 'equilibrium', 'relaxation', 'doe', 'occur', 'reason', 'system', 'ha', 'called', 'equilibrium', 'model', 'model', 'also', 'referred', 'restricted', 'coulomb', 'energy', 'rce', 'tm', 'model', 'share', 'property', 'unrestricted', 'storage', 'density', 'learning', 'high', 'density', 'memory', 'simple', 'learning', 'algorithm', 'placement', 'well', 'ha', 'described', 'detail', 'elsewhere', 'figure', 'layer', 'feedforward', 'network', 'cell', 'computes', 'quantity', 'lit', 'xil', 'compare', 'internal', 'threshold', 'reilly', 'et', 'al', 'employed', 'three', 'layer', 'feedforward', 'network', 'figure', 'allows', 'generalization', 'content', 'addressable', 'memory', 'pattern', 'classification', 'memory', 'location', 'minimum', 'explicitly', 'known', 'equilibrium', 'model', 'possible', 'dynamically', 'program', 'energy', 'function', 'arbitrary', 'energy', 'landscape', 'allows', 'construction', 'geography', 'basin', 'associated', 'class', 'constituting', 'pattern', 'environment', 'rapid', 'learning', 'complex', 'non', 'linear', 'disjoint', 'class', 'region', 'possible', 'method', 'learning', 'non', 'separable', 'class', 'region', 'previous', 'study', 'focused', 'acquisition', 'geography', 'boundary', 'non', 'linearly', 'separable', 'point', 'set', 'however', 'method', 'high', 'density', 'model', 'acquire', 'probability', 'distribution', 'non', 'separable', 'set', 'ha', 'described', 'non', 'separable', 'set', 'defined', 'point', 'set', 'state', 'space', 'system', 'labelled', 'multiple', 'class', 'affiliation', 'occur', 'input', 'space', 'ha', 'carried', 'feature', 'pattern', 'environment', 'pattern', 'set', 'separable', 'point', 'may', 'degenerate', 'respect', 'explicit', 'feature', 'space', 'however', 'may', 'different', 'probability', 'distribution', 'within', 'environment', 'structure', 'environment', 'important', 'information', 'identification', 'pattern', 'memory', 'presence', 'feature', 'space', 'degeneracy', 'describe', 'one', 'possible', 'mechanism', 'acquisition', 'probability', 'distribution', 'non', 'separable', 'point', 'assumed', 'point', 'region', 'state', 'space', 'network', 'site', 'event', 'ci', 'example', 'pattern', 'class', 'c1', 'cm', 'basin', 'attraction', 'xk', 'ci', 'defined', 'equation', 'placed', 'site', 'ci', 'unless', 'ci', 'xj', 'ci', 'ro', 'unless', 'memory', 'xj', 'class', 'ci', 'already', 'contains', 'ci', 'initial', 'value', 'qo', 'xk', 'ci', 'constant', 'site', 'xj', 'thus', 'event', 'class', 'c1', 'cm', 'occur', 'particular', 'site', 'multiple', 'well', 'placed', 'location', 'well', 'xj', 'ci', 'correctly', 'cover', 'event', 'ci', 'charge', 'site', 'defines', 'depth', 'well', 'incremented', 'constant', 'amount', 'qo', 'manner', 'region', 'covered', 'well', 'class', 'cm', 'depth', 'well', 'xj', 'ci', 'proportional', 'frequency', 'occurence', 'ci', 'xj', 'architecture', 'network', 'exactly', 'already', 'described', 'network', 'acquires', 'new', 'cell', 'well', 'placed', 'energy', 'landscape', 'thus', 'able', 'describe', 'meaning', 'well', 'overlap', 'competition', 'multiple', 'cell', 'layer', 'firing', 'pattern', 'activity', 'input', 'layer', 'application', 'system', 'ha', 'applied', 'problem', 'area', 'risk', 'assessment', 'mortgage', 'lending', 'input', 'space', 'consisted', 'feature', 'detector', 'continuous', 'firing', 'rate', 'proportional', 'value', 'variable', 'application', 'mortgage', 'set', 'feature', 'significant', 'portion', 'space', 'wa', 'non', 'separable', 'figure', '2a', '2b', 'illustrate', 'probability', 'distribution', 'high', 'low', 'risk', 'application', 'two', 'feature', 'clear', 'dimensional', 'subspace', 'region', 'high', 'low', 'risk', 'non', 'separable', 'different', 'distribution', 'feature', 'prob', 'pattern', 'prob', 'figure', '2a', 'probability', 'distribution', 'high', 'low', 'risk', 'pattern', 'feature', 'feature', 'prob', 'pattern', 'prob', 'figure', '2b', 'probability', 'distribution', 'high', 'low', 'risk', 'pattern', 'feature', 'figure', 'depicts', 'probability', 'distribution', 'acquired', 'system', 'dimensional', 'subspace', 'image', 'circle', 'radius', 'proportional', 'degree', 'risk', 'small', 'circle', 'region', 'low', 'risk', 'large', 'circle', 'region', 'high', 'risk', '0o', 'ooo', 'oo', 'oo', 'oo', 'oo', '0o', 'oo', 'feature', 'figure', 'probability', 'distribition', 'low', 'high', 'risk', 'small', 'circle', 'indicate', 'low', 'risk', 'region', 'large', 'circle', 'indicate', 'high', 'risk', 'region', 'particular', 'interest', 'clear', 'clustering', 'high', 'low', 'risk', 'region', 'map', 'note', 'region', 'fact', 'non', 'linearly', 'separable', 'discussion', 'presented', 'simple', 'method', 'acquisition', 'probability', 'distribution', 'non', 'separable', 'point', 'set', 'method', 'generates', 'energy', 'landscape', 'potential', 'well', 'depth', 'proportional', 'local', 'probability', 'density', 'class', 'pattern', 'environment', 'well', 'depth', 'set', 'probability', 'firing', 'class', 'cell', 'layer', 'feedforward', 'network', 'application', 'method', 'problem', 'risk', 'assessment', 'ha', 'shown', 'even', 'completely', 'non', 'separable', 'subspace', 'may', 'modeled', 'surprising', 'accuracy', 'method', 'improves', 'pattern', 'classification', 'problem', 'little', 'additional', 'computational', 'burden', 'algorithm', 'ha', 'run', 'conjunction', 'method', 'described', 'reilly', 'et', 'al', 'separable', 'region', 'combined', 'system', 'able', 'generate', 'non', 'linear', 'decision', 'surface', 'separable', 'zone', 'approximate', 'probability', 'distribution', 'non', 'separable', 'zone', 'seemless', 'manner', 'discussion', 'system', 'appear', 'future', 'report', 'current', 'work', 'focused', 'development', 'general', 'method', 'modelling', 'scale', 'variation', 'distribution', 'sensitivity', 'scale', 'suggests', 'transition', 'separable', 'non', 'separable', 'region', 'smooth', 'handled', 'hard', 'threshold', 'acknowi', 'edgements', 'would', 'like', 'thank', 'ed', 'collins', 'sushmito', 'ghosh', 'significant', 'contribution', 'work', 'development', 'mortgage', 'risk', 'assessment', 'application', 'reference', 'anderson', 'simple', 'neural', 'network', 'generating', 'interactive', 'memory', 'math', 'biosci', 'cooper', 'possible', 'organization', 'animal', 'memory', 'learning', 'proceeding', 'nobel', 'symposium', 'collective', 'property', 'physical', 'system', 'lundquist', 'lundquist', 'ed', 'london', 'new', 'york', 'academic', 'press', 'kohonen', 'correlation', 'matrix', 'memory', 'ieee', 'trans', 'comput', 'kohonen', 'associative', 'memory', 'system', 'theoretical', 'approach', 'berlin', 'heidelberg', 'new', 'york', 'springer', 'hopfield', 'neural', 'network', 'physical', 'system', 'emergent', 'collective', 'computational', 'ability', 'proc', 'natl', 'acad', 'sci', 'usa', 'april', 'hopfield', 'neuron', 'graded', 'response', 'collective', 'computational', 'property', 'like', 'two', 'state', 'neuron', 'proc', 'natl', 'acad', 'sci', 'usa', 'may', 'hopfield', 'feinstein', 'palmer', 'unlearning', 'ha', 'stabilizing', 'effect', 'collective', 'memory', 'nature', 'july', 'potter', 'ph', 'dissertation', 'advanced', 'technology', 'binghampton', 'unpublished', 'bachmann', 'cooper', 'dembo', 'zeitouni', 'relaxation', 'model', 'memory', 'high', 'density', 'storage', 'published', 'proc', 'natl', 'acad', 'sci', 'usa', 'dembo', 'zeitouni', 'aro', 'technical', 'report', 'brown', 'university', 'center', 'neural', 'science', 'providence', 'also', 'submitted', 'phys', 'rev', 'reilly', 'cooper', 'elbaum', 'neural', 'model', 'category', 'learning', 'biol', 'cybern', 'reilly', 'scofield', 'elbaum', 'cooper', 'learning', 'system', 'architecture', 'composed', 'multiple', 'learning', 'module', 'appear', 'proc', 'first', 'int', 'conf', 'neural', 'network', 'rimey', 'gouin', 'scofield', 'reilly', 'real', 'time', 'object', 'classification', 'using', 'learning', 'system', 'intelligent', 'robot', 'computer', 'vision', 'proc', 'spie', 'reilly', 'scofield', 'elbaum', 'cooper', 'neural', 'network', 'low', 'connectivity', 'unrestricted', 'memory', 'storage', 'density', 'published']\n",
            "CPU times: user 33.2 s, sys: 389 ms, total: 33.6 s\n",
            "Wall time: 36 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the bi-gram model"
      ],
      "metadata": {
        "id": "sj-Kj3o_qDHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before doing feature engineering and vectorization, it is time to extract some useful bi-gram based phrases from the text data and remove some unnecessary terms. We leverage the very useful gensim.models.Phrases class for this. This capability helps us automatically detect common phrases from a stream of sentences, which are typically multi-word expressions/word n-grams.\n",
        "\n",
        "We leverage the min_count parameter, which tells us that our model ignores all words and bi-grams with total collected count lower than 20 across the corpus (of the input paper as a list of tokenized sentences). We also use a threshold of 20, which tells us that the model accepts specific phrases based on this threshold value so that a phrase of words a followed by b is accepted if the score of the phrase is greater than the threshold of 20. This threshold is dependent on the scoring parameter, which helps us understand how these phrases are scored to understand their influence."
      ],
      "metadata": {
        "id": "v5upna7crpog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "#bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20,delimiter=b'_') # putting delimiter is giving error.\n",
        "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20) # higher threshold fewer phrases.\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "\n",
        "print(bigram_model[norm_papers[0][:100]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tpENLhWqN3X",
        "outputId": "b0e548f4-af5f-4dcd-f68a-129a1579bfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['patyep', 'class', 'degeneracy', 'unrestricted', 'storage', 'density', 'memory', 'christopher', 'scofield', 'douglas', 'reilly', 'charles', 'elbaum', 'leon', 'cooper', 'nestor', 'inc', 'richmond', 'square', 'providence', 'rhode', 'island', 'abstract', 'study', 'distributed', 'memory', 'system', 'ha', 'produced', 'number', 'model', 'work', 'well', 'limited', 'domain', 'however', 'recently', 'application', 'system', 'real_world', 'problem', 'ha', 'difficult', 'storage', 'limitation', 'inherent', 'architectural', 'serial', 'simulation', 'computational_complexity', 'recent', 'development', 'memory', 'unrestricted', 'storage_capacity', 'economical', 'feedforward', 'architecture', 'ha', 'opened', 'way', 'application', 'system', 'complex', 'pattern_recognition', 'problem', 'however', 'problem', 'sometimes', 'underspecified', 'feature', 'describe', 'environment', 'thus', 'significant', 'portion', 'pattern', 'environment', 'often', 'non', 'separable', 'review', 'current', 'work', 'high', 'density', 'memory', 'system', 'network', 'implementation', 'discus', 'general', 'learning', 'algorithm', 'high', 'density']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can now build the normalized bi-gram corpus"
      ],
      "metadata": {
        "id": "GSw7012CxqhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_corpus_bigram = [bigram_model[doc] for doc in norm_papers]\n",
        "dictionary = gensim.corpora.Dictionary(norm_corpus_bigram)\n",
        "\n",
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3CI1WrMx2Lg",
        "outputId": "55be9413-ae5d-4fdb-b923-cde3ab098173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample word to number mappings: [(0, '0o'), (1, '15n'), (2, '2b'), (3, '2n'), (4, '6introduced'), (5, 'ability'), (6, 'able'), (7, 'abstract'), (8, 'acad_sci'), (9, 'academic_press'), (10, 'according'), (11, 'accuracy'), (12, 'accurately'), (13, 'acknowi'), (14, 'acquire')]\n",
            "Total Vocabulary Size: 78892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, find out unique words found in documents. Filter out commonly used words."
      ],
      "metadata": {
        "id": "sVnImi43zF77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out words that occur less than 20 documents, or more than 60% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV7Z4tyuzNPZ",
        "outputId": "a66f99bd-723c-4295-bd0d-d0c1e6e4b2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Vocabulary Size: 7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigram]\n",
        "print(bow_corpus[1][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skjeE9bf1aAI",
        "outputId": "b7150070-e6f0-427b-81ca-76a8431defa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(5, 1), (6, 2), (16, 1), (17, 2), (20, 1), (22, 2), (23, 1), (27, 2), (29, 4), (30, 1), (35, 3), (36, 4), (39, 4), (56, 3), (62, 1), (73, 2), (74, 1), (84, 1), (88, 1), (89, 1), (91, 40), (94, 3), (95, 5), (97, 3), (102, 5), (106, 2), (109, 14), (117, 1), (118, 4), (120, 1), (124, 1), (125, 3), (126, 3), (130, 1), (132, 1), (145, 1), (162, 1), (163, 1), (165, 7), (171, 1), (176, 1), (178, 1), (192, 8), (193, 2), (199, 2), (201, 2), (216, 2), (219, 1), (227, 3), (228, 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jre65wL1hOy",
        "outputId": "c7d2cf26-5e11-4e9a-dbd3-d656a80b60d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('abstract', 1), ('acad_sci', 2), ('addition', 1), ('additional', 2), ('allows', 1), ('always', 2), ('american_institute', 1), ('another', 2), ('application', 4), ('applied', 1), ('architecture', 3), ('area', 4), ('associative_memory', 4), ('cell', 3), ('choice', 1), ('cm', 2), ('collective', 1), ('computational', 1), ('computing', 1), ('condition', 1), ('connection', 40), ('consists', 3), ('constant', 5), ('contains', 3), ('convergence', 5), ('could', 2), ('current', 14), ('degree', 1), ('density', 4), ('describe', 1), ('development', 1), ('difficult', 3), ('dimension', 3), ('directly', 1), ('discus', 1), ('effect', 1), ('fact', 1), ('feature', 1), ('feedback', 7), ('finally', 1), ('flow', 1), ('found', 1), ('high', 8), ('hopfield', 2), ('implementation', 2), ('important', 2), ('interest', 2), ('investigation', 1), ('larger', 3), ('layer', 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total number of papers:', len(bow_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-M6oi_r1nOm",
        "outputId": "9a75ad35-c86a-44a2-cfba-b14ebf737982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of papers: 1740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find out TOPICS"
      ],
      "metadata": {
        "id": "gEX22PmLAwtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "TOTAL_TOPICS = 10\n",
        "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
        "                                 onepass=True, chunksize=1740, power_iters=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcnatjmeAyiW",
        "outputId": "911dda26-5005-412f-c3c1-e522586b7767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 33s, sys: 1min 46s, total: 8min 20s\n",
            "Wall time: 5min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0hGDnuVA5ZE",
        "outputId": "a9239ffe-9b91-4e02-c32d-21b1037f2b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "0.215*\"unit\" + 0.212*\"state\" + 0.187*\"training\" + 0.177*\"neuron\" + 0.162*\"pattern\" + 0.145*\"image\" + 0.140*\"vector\" + 0.125*\"feature\" + 0.122*\"cell\" + 0.110*\"layer\" + 0.101*\"task\" + 0.097*\"class\" + 0.091*\"probability\" + 0.089*\"signal\" + 0.087*\"step\" + 0.086*\"response\" + 0.085*\"representation\" + 0.083*\"noise\" + 0.082*\"rule\" + 0.081*\"distribution\"\n",
            "\n",
            "Topic #2:\n",
            "0.487*\"neuron\" + 0.396*\"cell\" + -0.257*\"state\" + 0.191*\"response\" + -0.187*\"training\" + 0.170*\"stimulus\" + 0.117*\"activity\" + -0.109*\"class\" + 0.099*\"spike\" + 0.097*\"pattern\" + 0.096*\"circuit\" + 0.096*\"synaptic\" + -0.095*\"vector\" + 0.090*\"signal\" + 0.090*\"firing\" + 0.088*\"visual\" + -0.084*\"classifier\" + -0.083*\"action\" + -0.078*\"word\" + 0.078*\"cortical\"\n",
            "\n",
            "Topic #3:\n",
            "-0.627*\"state\" + 0.395*\"image\" + -0.219*\"neuron\" + 0.209*\"feature\" + -0.188*\"action\" + 0.137*\"unit\" + 0.131*\"object\" + -0.130*\"control\" + 0.129*\"training\" + -0.109*\"policy\" + 0.103*\"classifier\" + 0.090*\"class\" + -0.081*\"step\" + -0.081*\"dynamic\" + 0.080*\"classification\" + 0.078*\"layer\" + 0.076*\"recognition\" + -0.074*\"reinforcement_learning\" + 0.069*\"representation\" + 0.068*\"pattern\"\n",
            "\n",
            "Topic #4:\n",
            "0.686*\"unit\" + -0.433*\"image\" + 0.182*\"pattern\" + 0.131*\"layer\" + 0.123*\"hidden_unit\" + 0.121*\"net\" + 0.114*\"training\" + -0.112*\"feature\" + 0.109*\"activation\" + 0.107*\"rule\" + -0.097*\"neuron\" + 0.078*\"word\" + -0.070*\"pixel\" + 0.070*\"connection\" + -0.067*\"object\" + -0.065*\"state\" + -0.060*\"distribution\" + -0.059*\"face\" + 0.057*\"architecture\" + -0.055*\"estimate\"\n",
            "\n",
            "Topic #5:\n",
            "0.428*\"image\" + 0.348*\"state\" + -0.266*\"neuron\" + 0.264*\"unit\" + -0.181*\"training\" + -0.174*\"class\" + 0.168*\"object\" + -0.167*\"classifier\" + 0.147*\"action\" + 0.122*\"visual\" + -0.117*\"vector\" + -0.115*\"node\" + -0.105*\"distribution\" + 0.103*\"motion\" + 0.099*\"feature\" + -0.097*\"classification\" + 0.097*\"control\" + 0.095*\"task\" + 0.087*\"cell\" + 0.083*\"representation\"\n",
            "\n",
            "Topic #6:\n",
            "0.660*\"cell\" + -0.508*\"neuron\" + -0.213*\"image\" + -0.103*\"chip\" + -0.097*\"unit\" + 0.093*\"response\" + -0.090*\"object\" + 0.083*\"rat\" + 0.076*\"distribution\" + -0.070*\"circuit\" + 0.069*\"probability\" + 0.064*\"stimulus\" + -0.061*\"memory\" + -0.058*\"analog\" + -0.058*\"activation\" + 0.055*\"class\" + -0.053*\"bit\" + -0.052*\"net\" + 0.051*\"cortical\" + 0.050*\"firing\"\n",
            "\n",
            "Topic #7:\n",
            "-0.353*\"word\" + 0.281*\"unit\" + -0.272*\"training\" + -0.257*\"classifier\" + -0.177*\"recognition\" + 0.159*\"distribution\" + -0.152*\"feature\" + -0.144*\"state\" + -0.142*\"pattern\" + 0.141*\"vector\" + -0.128*\"cell\" + -0.128*\"task\" + 0.122*\"approximation\" + 0.121*\"variable\" + 0.110*\"equation\" + -0.107*\"classification\" + 0.106*\"noise\" + -0.103*\"class\" + 0.101*\"matrix\" + -0.098*\"neuron\"\n",
            "\n",
            "Topic #8:\n",
            "0.303*\"pattern\" + -0.243*\"signal\" + -0.236*\"control\" + -0.202*\"training\" + 0.181*\"rule\" + 0.178*\"state\" + -0.167*\"noise\" + 0.166*\"class\" + -0.162*\"word\" + 0.155*\"cell\" + 0.154*\"feature\" + -0.147*\"motion\" + -0.140*\"task\" + 0.127*\"node\" + 0.124*\"neuron\" + -0.116*\"target\" + -0.114*\"circuit\" + 0.114*\"probability\" + 0.110*\"classifier\" + 0.109*\"image\"\n",
            "\n",
            "Topic #9:\n",
            "0.472*\"node\" + 0.254*\"circuit\" + -0.214*\"word\" + 0.201*\"chip\" + -0.190*\"neuron\" + -0.172*\"stimulus\" + 0.160*\"classifier\" + 0.152*\"current\" + -0.147*\"feature\" + 0.146*\"voltage\" + -0.145*\"distribution\" + 0.141*\"control\" + 0.124*\"rule\" + 0.110*\"layer\" + 0.105*\"analog\" + 0.091*\"tree\" + -0.084*\"response\" + -0.080*\"state\" + -0.079*\"probability\" + -0.079*\"estimate\"\n",
            "\n",
            "Topic #10:\n",
            "-0.518*\"word\" + 0.254*\"training\" + -0.236*\"vector\" + 0.222*\"task\" + 0.194*\"pattern\" + 0.156*\"classifier\" + -0.149*\"node\" + -0.146*\"recognition\" + 0.139*\"control\" + -0.138*\"sequence\" + 0.126*\"rule\" + -0.125*\"circuit\" + -0.123*\"cell\" + 0.113*\"action\" + 0.105*\"neuron\" + -0.094*\"hmm\" + -0.093*\"character\" + -0.088*\"chip\" + -0.088*\"matrix\" + -0.085*\"structure\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbdvFbRX3g8Z",
        "outputId": "4a362f98-ead7-4b6e-c702-160980166b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('unit', 0.215), ('state', 0.212), ('training', 0.187), ('neuron', 0.177), ('pattern', 0.162), ('image', 0.145), ('vector', 0.14), ('feature', 0.125), ('cell', 0.122), ('layer', 0.11), ('task', 0.101), ('class', 0.097), ('probability', 0.091), ('signal', 0.089), ('step', 0.087), ('response', 0.086), ('representation', 0.085), ('noise', 0.083), ('rule', 0.082), ('distribution', 0.081)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('neuron', 0.487), ('cell', 0.396), ('response', 0.191), ('stimulus', 0.17), ('activity', 0.117), ('spike', 0.099), ('pattern', 0.097), ('circuit', 0.096), ('synaptic', 0.096), ('signal', 0.09), ('firing', 0.09), ('visual', 0.088), ('cortical', 0.078)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('state', -0.257), ('training', -0.187), ('class', -0.109), ('vector', -0.095), ('classifier', -0.084), ('action', -0.083), ('word', -0.078)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.395), ('feature', 0.209), ('unit', 0.137), ('object', 0.131), ('training', 0.129), ('classifier', 0.103), ('class', 0.09), ('classification', 0.08), ('layer', 0.078), ('recognition', 0.076), ('representation', 0.069), ('pattern', 0.068)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('state', -0.627), ('neuron', -0.219), ('action', -0.188), ('control', -0.13), ('policy', -0.109), ('step', -0.081), ('dynamic', -0.081), ('reinforcement_learning', -0.074)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('unit', 0.686), ('pattern', 0.182), ('layer', 0.131), ('hidden_unit', 0.123), ('net', 0.121), ('training', 0.114), ('activation', 0.109), ('rule', 0.107), ('word', 0.078), ('connection', 0.07), ('architecture', 0.057)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -0.433), ('feature', -0.112), ('neuron', -0.097), ('pixel', -0.07), ('object', -0.067), ('state', -0.065), ('distribution', -0.06), ('face', -0.059), ('estimate', -0.055)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.428), ('state', 0.348), ('unit', 0.264), ('object', 0.168), ('action', 0.147), ('visual', 0.122), ('motion', 0.103), ('feature', 0.099), ('control', 0.097), ('task', 0.095), ('cell', 0.087), ('representation', 0.083)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('neuron', -0.266), ('training', -0.181), ('class', -0.174), ('classifier', -0.167), ('vector', -0.117), ('node', -0.115), ('distribution', -0.105), ('classification', -0.097)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('cell', 0.66), ('response', 0.093), ('rat', 0.083), ('distribution', 0.076), ('probability', 0.069), ('stimulus', 0.064), ('class', 0.055), ('cortical', 0.051), ('firing', 0.05)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('neuron', -0.508), ('image', -0.213), ('chip', -0.103), ('unit', -0.097), ('object', -0.09), ('circuit', -0.07), ('memory', -0.061), ('analog', -0.058), ('activation', -0.058), ('bit', -0.053), ('net', -0.052)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('unit', 0.281), ('distribution', 0.159), ('vector', 0.141), ('approximation', 0.122), ('variable', 0.121), ('equation', 0.11), ('noise', 0.106), ('matrix', 0.101)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.353), ('training', -0.272), ('classifier', -0.257), ('recognition', -0.177), ('feature', -0.152), ('state', -0.144), ('pattern', -0.142), ('cell', -0.128), ('task', -0.128), ('classification', -0.107), ('class', -0.103), ('neuron', -0.098)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('pattern', 0.303), ('rule', 0.181), ('state', 0.178), ('class', 0.166), ('cell', 0.155), ('feature', 0.154), ('node', 0.127), ('neuron', 0.124), ('probability', 0.114), ('classifier', 0.11), ('image', 0.109)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('signal', -0.243), ('control', -0.236), ('training', -0.202), ('noise', -0.167), ('word', -0.162), ('motion', -0.147), ('task', -0.14), ('target', -0.116), ('circuit', -0.114)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('node', 0.472), ('circuit', 0.254), ('chip', 0.201), ('classifier', 0.16), ('current', 0.152), ('voltage', 0.146), ('control', 0.141), ('rule', 0.124), ('layer', 0.11), ('analog', 0.105), ('tree', 0.091)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.214), ('neuron', -0.19), ('stimulus', -0.172), ('feature', -0.147), ('distribution', -0.145), ('response', -0.084), ('state', -0.08), ('probability', -0.079), ('estimate', -0.079)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('training', 0.254), ('task', 0.222), ('pattern', 0.194), ('classifier', 0.156), ('control', 0.139), ('rule', 0.126), ('action', 0.113), ('neuron', 0.105)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.518), ('vector', -0.236), ('node', -0.149), ('recognition', -0.146), ('sequence', -0.138), ('circuit', -0.125), ('cell', -0.123), ('hmm', -0.094), ('character', -0.093), ('chip', -0.088), ('matrix', -0.088), ('structure', -0.085)]\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z26igUzF3pkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSI works with SVD ( Singular Value Decomsition) principle. M is typically known as the term-document matrix and is usually obtained after feature engineering on the preprocessed text data, where each row of the matrix represents a term and each column represents a text document.\n",
        "\n",
        "U is known as the term-topic matrix where each row of the matrix represents a term and each column represents a topic. It’s useful for getting the influential terms for each topic when we multiply this by the singular values.\n",
        "\n",
        "S is the matrix or array that consists of the list of singular values obtained after low-rank SVD, which is typically equal to the number of topics we decide prior to this operation.\n",
        "\n",
        "VT is the topic-document matrix, which if you transpose, you get the document-topic matrix, which is useful in knowing how much influence each topic has on each document."
      ],
      "metadata": {
        "id": "YxPDISu03yhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term_topic = lsi_bow.projection.u\n",
        "singular_values = lsi_bow.projection.s\n",
        "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
        "term_topic.shape, singular_values.shape, topic_document.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9RRH89X36MQ",
        "outputId": "3d7fad39-8719-4137-920c-9f80e4b81038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7756, 10), (10,), (10, 1740))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a term-topic matrix, singular values, and a topic-document matrix. We can transpose the topic-document matrix to form a documenttopic matrix and that would help us see the proportion of each topic per document (a larger proportion means the topic is more dominant in the document)."
      ],
      "metadata": {
        "id": "E8hF-29t4MKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
        "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "document_topics.head(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "7471V1ov4QVU",
        "outputId": "e4031958-d477-4b29-f2c0-d0b53a3485a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
              "0   0.021  0.005 -0.006 -0.002  0.002  0.006 -0.014  0.040  0.002  0.002\n",
              "1   0.028  0.050 -0.031 -0.016 -0.028 -0.067 -0.010 -0.006  0.058 -0.026\n",
              "2   0.030  0.046 -0.011 -0.026 -0.057 -0.046 -0.017  0.009 -0.048  0.017\n",
              "3   0.017 -0.005 -0.004  0.000 -0.015 -0.006  0.004  0.011  0.001 -0.038\n",
              "4   0.020 -0.005  0.014  0.008 -0.018 -0.008 -0.047 -0.011 -0.014 -0.017\n",
              "5   0.028  0.038  0.011 -0.010  0.016  0.071 -0.001  0.008  0.004 -0.010\n",
              "6   0.017  0.020 -0.007 -0.013 -0.003  0.030  0.009 -0.009  0.035 -0.017\n",
              "7   0.022  0.049 -0.011 -0.004 -0.013  0.031 -0.011  0.013  0.001 -0.013\n",
              "8   0.029  0.052  0.001  0.021 -0.003  0.055 -0.041  0.052  0.019  0.013\n",
              "9   0.016  0.004 -0.012 -0.001 -0.007 -0.015  0.012  0.007  0.003 -0.002\n",
              "10  0.015  0.023 -0.010  0.005 -0.010 -0.013  0.001  0.005  0.004 -0.004\n",
              "11  0.019  0.006 -0.003  0.000 -0.005  0.011  0.017  0.007  0.005 -0.011\n",
              "12  0.025 -0.001  0.015  0.039  0.006 -0.007  0.026  0.006  0.007  0.002\n",
              "13  0.055 -0.006  0.017  0.086  0.105 -0.022  0.030 -0.002 -0.005  0.037\n",
              "14  0.021  0.021 -0.034 -0.015 -0.030 -0.049  0.002  0.036 -0.020  0.006"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d298ac8-9b9d-4921-b865-d3c3a3434ca2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.021</td>\n",
              "      <td>0.005</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.028</td>\n",
              "      <td>0.050</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>-0.028</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.058</td>\n",
              "      <td>-0.026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.030</td>\n",
              "      <td>0.046</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.026</td>\n",
              "      <td>-0.057</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.009</td>\n",
              "      <td>-0.048</td>\n",
              "      <td>0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.001</td>\n",
              "      <td>-0.038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.018</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.047</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.028</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.071</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.017</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.009</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.035</td>\n",
              "      <td>-0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.022</td>\n",
              "      <td>0.049</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>0.031</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>-0.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.029</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.041</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.016</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.015</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>0.005</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.019</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.005</td>\n",
              "      <td>-0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.025</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.105</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.030</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.021</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.034</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.030</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.036</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d298ac8-9b9d-4921-b865-d3c3a3434ca2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d298ac8-9b9d-4921-b865-d3c3a3434ca2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d298ac8-9b9d-4921-b865-d3c3a3434ca2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_paper_patterns = ['Feudal Reinforcement Learning \\nPeter', 'Illumination-Invariant Face Recognition with a', 'Improved Hidden Markov Model Speech Recognition']\n",
        "sample_paper_idxs = [idx for pattern in sample_paper_patterns \n",
        "                            for idx, content in enumerate(papers) \n",
        "                                if pattern in content]\n",
        "sample_paper_idxs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGacS2E05uql",
        "outputId": "e7d14bbb-03dc-4e21-ca64-aec9129b51b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[670, 728, 558]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_numbers = sample_paper_idxs\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMFpOIbM5zmn",
        "outputId": "a2062a7d-edf8-452e-9a6f-e2f6f31b525f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document #670:\n",
            "Dominant Topics (top 3): ['T5', 'T3', 'T10']\n",
            "Paper Summary:\n",
            "Feudal Reinforcement Learning \n",
            "Peter Dayan \n",
            "CNL \n",
            "The Salk Institute \n",
            "PO Box 85800 \n",
            "San Diego CA 92186-5800, USA \n",
            "dayanhelmholtz. sdsc. edu \n",
            "Geoffrey E Hinton \n",
            "Department of Computer Science \n",
            "University of Toronto \n",
            "6 Kings College Road, Toronto, \n",
            "Canada M5S 1A4 \n",
            "hintonai. toronto. edu \n",
            "Abstract \n",
            "One way to speed up reinforcement learning is to enable learning to \n",
            "happen simultaneously at multiple resolutions in space and time. \n",
            "This paper shows how to create a Q-learning managerial hierarchy \n",
            "i\n",
            "\n",
            "Document #728:\n",
            "Dominant Topics (top 3): ['T4', 'T3', 'T5']\n",
            "Paper Summary:\n",
            "Illumination-Invariant Face Recognition with a \n",
            "Contrast Sensitive Silicon Retina \n",
            "Joachim M. Buhmann \n",
            "Rheinische Friedrich-Wilhelms-Universitfit \n",
            "Institut ftir Informatik II, R6merstrage 164 \n",
            "D-53117 Bonn, Germany \n",
            "Martin Lades \n",
            "Ruhr-Universitfit Bochum \n",
            "Institut ftir Neuroinformatik \n",
            "D-44780 Bochum, Germany \n",
            "Frank Eeckman \n",
            "Lawrence Livermore National Laboratory \n",
            "ISCR, P.O.Box 808, L-426 \n",
            "Livermore, CA 94551 \n",
            "Abstract \n",
            "Changes in lighting conditions strongly effect the performance and reli- \n",
            "ab\n",
            "\n",
            "Document #558:\n",
            "Dominant Topics (top 3): ['T7', 'T10', 'T2']\n",
            "Paper Summary:\n",
            "Improved Hidden Markov Model \n",
            "Speech Recognition Using \n",
            "Radial Basis Function Networks \n",
            "Elliot Singer and Richard P. Lippmann \n",
            "Lincoln Laboratory, MIT \n",
            "Lexington, MA 02173-9108, USA \n",
            "Abstract \n",
            "A high performance speaker-independent isolated-word hybrid speech rec- \n",
            "ognizer was developed which combines Hidden Markov Models (HMMs) \n",
            "and Radial Basis Function (RBF) neural networks. In recognition ex- \n",
            "periments using a speaker-independent E-set database, the hybrid rec- \n",
            "ognizer had an error rate of\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LEt us now Build the LSI model from beginning for our corpus"
      ],
      "metadata": {
        "id": "UMdfyzXk58Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on what we mentioned earlier, the heart of LSI models involves Singular Value Decomposition (SVD). Here, we try to implement an LSI topic model from scratch using low-rank SVD. The first step in SVD is to get the source matrix, which is typically a term-document matrix. We can obtain it from Gensim by converting the sparse Bag of Words representation into a dense matrix."
      ],
      "metadata": {
        "id": "QmdEgkME6bBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td_matrix = gensim.matutils.corpus2dense(corpus=bow_corpus, num_terms=len(dictionary))\n",
        "print(td_matrix.shape)\n",
        "td_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c1rvMgb6DOO",
        "outputId": "06809f95-4111-46a7-a525-b1fec68c4917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7756, 1740)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 1., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = np.array(list(dictionary.values()))\n",
        "print('Total vocabulary size:', len(vocabulary))\n",
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twt-7yJG6_JR",
        "outputId": "d7ae53d6-d53c-4f33-e2a8-a0f5840464f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vocabulary size: 7756\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0o', '2b', '2n', ..., 'support_vector', 'mozer_jordan',\n",
              "       'kearns_solla'], dtype='<U28')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jNjhV3u37G87",
        "outputId": "1b1f2fd7-96d6-4834-ad7a-c30a82e15328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contrast'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now perform the singular Value Decomposition"
      ],
      "metadata": {
        "id": "wdALjPRf7ROI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "u, s, vt = svds(td_matrix, k=TOTAL_TOPICS, maxiter=10000)\n",
        "term_topic = u\n",
        "singular_values = s\n",
        "topic_document = vt\n",
        "term_topic.shape, singular_values.shape, topic_document.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8kR_3g67PKG",
        "outputId": "d24203d5-0242-4f2a-f4bc-d64a15ce91da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7756, 10), (10,), (10, 1740))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(singular_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj3Vn1YY7eTM",
        "outputId": "e1c6efcc-9222-4870-ba5a-808632733b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 364.19394  366.74716  385.9672   418.6218   432.89566  489.67987\n",
            "  498.0195   580.7572   628.6302  1215.5349 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tt_weights = term_topic.transpose() * singular_values[:, None]\n",
        "tt_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5UqFSoI7p5g",
        "outputId": "80c93c9e-240a-4131-9f7f-69b1eddb20c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 7756)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_terms = 20\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(tt_weights), axis=1)[:, :top_terms]\n",
        "topic_keyterm_weights = np.array([tt_weights[row, columns] \n",
        "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    terms, weights = topic_keyterms_weights[n]\n",
        "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
        "                          key=lambda row: -abs(row[1]))\n",
        "    for term, wt in term_weights:\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaT464Wb9-yv",
        "outputId": "836095cd-d87e-4179-c661-50c261b389fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('word', 188.488), ('vector', 85.974), ('node', 54.374), ('recognition', 53.232), ('sequence', 50.351), ('circuit', 45.393), ('cell', 44.81), ('hmm', 34.086), ('character', 34.022), ('chip', 32.159), ('matrix', 32.093), ('structure', 30.993)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('training', -92.618), ('task', -80.732), ('pattern', -70.618), ('classifier', -56.989), ('control', -50.677), ('rule', -45.927), ('action', -41.202), ('neuron', -38.193)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('word', 78.345), ('neuron', 69.793), ('stimulus', 63.234), ('feature', 53.819), ('distribution', 53.119), ('response', 30.954), ('state', 29.343), ('probability', 29.099), ('estimate', 28.908)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('node', -173.277), ('circuit', -93.0), ('chip', -73.594), ('classifier', -58.716), ('current', -55.844), ('voltage', -53.49), ('control', -51.707), ('rule', -45.293), ('layer', -40.265), ('analog', -38.344), ('tree', -33.483)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('pattern', 116.971), ('rule', 69.783), ('state', 68.605), ('class', 64.259), ('cell', 59.979), ('feature', 59.606), ('node', 49.176), ('neuron', 47.998), ('probability', 43.812), ('classifier', 42.612), ('image', 42.061)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('signal', -93.805), ('control', -91.041), ('training', -77.88), ('noise', -64.397), ('word', -62.392), ('motion', -56.699), ('task', -53.882), ('target', -44.765), ('circuit', -44.129)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('unit', 117.727), ('distribution', 66.719), ('vector', 58.881), ('approximation', 50.931), ('variable', 50.83), ('equation', 46.229), ('noise', 44.248), ('matrix', 42.214)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -147.792), ('training', -113.693), ('classifier', -107.386), ('recognition', -73.948), ('feature', -63.454), ('state', -60.126), ('pattern', -59.561), ('cell', -53.769), ('task', -53.693), ('classification', -44.936), ('class', -43.161), ('neuron', -41.091)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('cell', 285.803), ('response', 40.216), ('rat', 35.975), ('distribution', 33.085), ('probability', 29.79), ('stimulus', 27.789), ('class', 24.02), ('cortical', 22.185), ('firing', 21.66)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('neuron', -220.116), ('image', -92.39), ('chip', -44.422), ('unit', -41.922), ('object', -39.001), ('circuit', -30.444), ('memory', -26.475), ('analog', -25.207), ('activation', -24.953), ('bit', -22.997), ('net', -22.699)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('neuron', 130.053), ('training', 88.668), ('class', 85.213), ('classifier', 81.92), ('vector', 57.531), ('node', 56.342), ('distribution', 51.622), ('classification', 47.645)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -209.794), ('state', -170.207), ('unit', -129.107), ('object', -82.185), ('action', -72.136), ('visual', -59.502), ('motion', -50.605), ('feature', -48.665), ('control', -47.427), ('task', -46.496), ('cell', -42.366), ('representation', -40.564)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('unit', 341.829), ('pattern', 90.771), ('layer', 65.337), ('hidden_unit', 61.12), ('net', 60.035), ('training', 56.741), ('activation', 54.268), ('rule', 53.377), ('word', 38.903), ('connection', 34.618), ('architecture', 28.439)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -215.857), ('feature', -55.647), ('neuron', -48.495), ('pixel', -35.095), ('object', -33.585), ('state', -32.543), ('distribution', -29.977), ('face', -29.256), ('estimate', -27.555)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('state', 364.388), ('neuron', 127.022), ('action', 109.245), ('control', 75.369), ('policy', 63.103), ('step', 47.226), ('dynamic', 46.907), ('reinforcement_learning', 42.747)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -229.286), ('feature', -121.396), ('unit', -79.441), ('object', -76.204), ('training', -75.153), ('classifier', -59.872), ('class', -52.527), ('classification', -46.696), ('layer', -45.149), ('recognition', -44.192), ('representation', -40.179), ('pattern', -39.252)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('neuron', 306.151), ('cell', 249.242), ('response', 119.758), ('stimulus', 106.762), ('activity', 73.499), ('spike', 62.039), ('pattern', 60.957), ('circuit', 60.603), ('synaptic', 60.282), ('signal', 56.665), ('firing', 56.597), ('visual', 55.571), ('cortical', 48.867)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('state', -161.465), ('training', -117.319), ('class', -68.732), ('vector', -59.558), ('classifier', -52.589), ('action', -52.113), ('word', -49.239)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: []\n",
            "--------------------------------------------------\n",
            "Direction 2: [('unit', -260.793), ('state', -258.146), ('training', -227.312), ('neuron', -215.681), ('pattern', -197.232), ('image', -175.735), ('vector', -170.154), ('feature', -151.546), ('cell', -148.139), ('layer', -133.593), ('task', -122.389), ('class', -117.848), ('probability', -110.526), ('signal', -108.232), ('step', -105.202), ('response', -104.465), ('representation', -103.255), ('noise', -100.573), ('rule', -99.611), ('distribution', -98.973)]\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}